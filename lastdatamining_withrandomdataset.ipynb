{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saimasharleen/Book-Recommendation-System/blob/main/lastdatamining_withrandomdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoRFxYML6NXw",
        "outputId": "f161de91-a0ed-469e-b095-41f3a5ffd3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.8/dist-packages (16.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.8/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install faker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0OyKpsMz6zwi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "with open('random_books.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13', 'language_code', 'num_pages', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher'])\n",
        "    for i in range(20000):\n",
        "        writer.writerow([i, fake.catch_phrase(), fake.name(), fake.random_number(digits=2), fake.isbn10(separator=\"-\"), fake.isbn13(separator=\"-\"), fake.language_code(), fake.random_number(digits=3), fake.random_number(digits=5), fake.random_number(digits=3), fake.date_this_decade(), fake.company()])\n",
        "\n",
        "os.rename('random_books.csv', 'book_dataset.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI5vxYmp-Oo7",
        "outputId": "7a8440d4-55dc-476f-bc4c-ca5f2651724b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of column names :  ['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13', 'language_code', 'num_pages', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher']\n"
          ]
        }
      ],
      "source": [
        "# importing the csv library\n",
        "import csv\n",
        " \n",
        "# opening the csv file by specifying\n",
        "# the location\n",
        "# with the variable name as csv_file\n",
        "with open('book_dataset.csv') as csv_file:\n",
        " \n",
        "    # creating an object of csv reader\n",
        "    # with the delimiter as ,\n",
        "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
        " \n",
        "    # list to store the names of columns\n",
        "    list_of_column_names = []\n",
        " \n",
        "    # loop to iterate through the rows of csv\n",
        "    for row in csv_reader:\n",
        " \n",
        "        # adding the first row\n",
        "        list_of_column_names.append(row)\n",
        " \n",
        "        # breaking the loop after the\n",
        "        # first iteration itself\n",
        "        break\n",
        " \n",
        "# printing the result\n",
        "print(\"List of column names : \",\n",
        "      list_of_column_names[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Enb1jDX4TZPX"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "import pandas as pd\n",
        "\n",
        "user_ids = set()\n",
        "while len(user_ids) < 20000:\n",
        "    user_ids.add(str(uuid.uuid4()))\n",
        "\n",
        "# convert the set to dataframe\n",
        "user_ids_df = pd.DataFrame(list(user_ids), columns=[\"user_id\"])\n",
        "\n",
        "# write the dataframe to a csv file\n",
        "user_ids_df.to_csv(\"user_ids.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8_XVWD3sTcW1"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "\n",
        "# Column names of the book dataset\n",
        "column_names = ['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13', 'language_code', 'num_pages', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher']\n",
        "\n",
        "# Number of queries to be generated\n",
        "num_queries = 1000\n",
        "\n",
        "# List to store queries\n",
        "queries = []\n",
        "\n",
        "# Open the book dataset CSV file\n",
        "with open('book_dataset.csv', 'r') as book_file:\n",
        "    # Create a CSV reader\n",
        "    reader = csv.DictReader(book_file, fieldnames=column_names)\n",
        "    # Iterate through rows of the book dataset\n",
        "    for i, row in enumerate(reader):\n",
        "        # Generate a query ID\n",
        "        query_id = 'Q' + str(i)\n",
        "        # Create a list to store the conditions\n",
        "        conditions = []\n",
        "        # Iterate through the columns of the book dataset\n",
        "        for column_name in column_names:\n",
        "            # Skip the first column (bookID)\n",
        "            if column_name == 'bookID':\n",
        "                continue\n",
        "            # Add the condition to the list\n",
        "            conditions.append(column_name + '=' + row[column_name])\n",
        "        # Join the conditions with comma separator\n",
        "        conditions_str = ','.join(conditions)\n",
        "        # Append the query to the list\n",
        "        queries.append([query_id, conditions_str])\n",
        "        # Break the loop after generating the specified number of queries\n",
        "        if i == num_queries:\n",
        "            break\n",
        "\n",
        "# Write the queries to a CSV file\n",
        "with open('queries.csv', 'w', newline='') as query_file:\n",
        "    writer = csv.writer(query_file)\n",
        "    writer.writerows(queries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2wupkyg_TsEC"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "import uuid\n",
        "\n",
        "# create the set of user IDs\n",
        "user_ids = set()\n",
        "while len(user_ids) < 1000:\n",
        "    user_ids.add(str(uuid.uuid4()))\n",
        "    \n",
        "# create the set of query IDs\n",
        "query_ids = set()\n",
        "while len(query_ids) < 10000:\n",
        "    query_ids.add(\"Q\"+str(uuid.uuid4()))\n",
        "\n",
        "# create the utility matrix\n",
        "utility_matrix = []\n",
        "\n",
        "# add the first row (query IDs)\n",
        "utility_matrix.append(list(query_ids))\n",
        "\n",
        "# add the rest of the rows (user IDs and random values)\n",
        "for user_id in user_ids:\n",
        "    row = [user_id]\n",
        "    for query_id in query_ids:\n",
        "        if random.random() < 0.5:\n",
        "            row.append(random.randint(1,100))\n",
        "        else:\n",
        "            row.append('')\n",
        "    utility_matrix.append(row)\n",
        "\n",
        "# write the utility matrix to a CSV file\n",
        "with open('utility_matrix.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(utility_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "arXfUvShULYM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix.csv')\n",
        "df.fillna(0, inplace=True)  # Replace NaN values with 0\n",
        "\n",
        "cosine_sim = cosine_similarity(df, dense_output=False)\n",
        "\n",
        "# Define the number of closest neighbors to use\n",
        "k = 10\n",
        "\n",
        "# Next, fill in missing values with estimated values using the cosine similarity matrix\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            user_ratings = df.iloc[i, :]\n",
        "            item_ratings = df.iloc[:, j]\n",
        "            user_ratings = user_ratings[~pd.isna(user_ratings)]\n",
        "            item_ratings = item_ratings[~pd.isna(item_ratings)]\n",
        "            # subtract the mean rating of each user and item\n",
        "            mean_user_rating = user_ratings.mean()\n",
        "            mean_item_rating = item_ratings.mean()\n",
        "            user_ratings -= mean_user_rating\n",
        "            item_ratings -= mean_item_rating\n",
        "            similar_users = cosine_sim[i][~pd.isna(df.iloc[:, j])]\n",
        "            similar_items = cosine_sim[j][~pd.isna(df.iloc[i, :])]\n",
        "            # get the k closest neighbors\n",
        "            top_k_users = similar_users.argsort()[-k:]\n",
        "            top_k_items = similar_items.argsort()[-k:]\n",
        "            similar_users = similar_users[top_k_users]\n",
        "            similar_items = similar_items[top_k_items]\n",
        "            item_ratings = item_ratings[top_k_items]\n",
        "            df.iloc[i, j] = mean_item_rating + mean_user_rating + (sum(similar_users * similar_items * item_ratings) / sum(similar_users * similar_items))\n",
        "\n",
        "# save the filled matrix to csv\n",
        "df.to_csv('utility_matrix_content_based.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY7jZ1AUdIzs",
        "outputId": "089ef2ac-9a71-498b-8f82-cec6254187d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of content solution: 35.69507555697578\n",
            "MAE of content solution: 25.23807597654161\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# Create an imputer with strategy 'mean'\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "  \n",
        "# Fit the imputer on the original data frame\n",
        "df_original = pd.read_csv('utility_matrix.csv')\n",
        "df_original = mean_imputer.fit_transform(df_original)\n",
        "\n",
        "# Read the content utility matrix from the CSV file\n",
        "df_con = pd.read_csv('utility_matrix_content_based.csv')\n",
        "df_con = mean_imputer.transform(df_con)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(df_original, df_con, squared=False)\n",
        "print(\"RMSE of content solution:\", rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(df_original, df_con)\n",
        "print(\"MAE of content solution:\", mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "89ayx61Bed1n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix.csv')\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# Compute the Pearson correlation coefficient between each pair of users\n",
        "user_corr = df.corr(method='pearson')\n",
        "\n",
        "# Fill in missing values with estimated values using collaborative filtering with Pearson correlation\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            user_ratings = df.iloc[i, :]\n",
        "            item_ratings = df.iloc[:, j]\n",
        "            user_ratings = user_ratings[~pd.isna(user_ratings)]\n",
        "            item_ratings = item_ratings[~pd.isna(item_ratings)]\n",
        "            similar_users = user_corr[i][~pd.isna(df.iloc[:, j])]\n",
        "            df.iloc[i, j] = (similar_users * item_ratings).sum() / similar_users.abs().sum()\n",
        "\n",
        "# save the filled matrix to csv\n",
        "df.to_csv('utility_matrix_collaborative_based.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZMRlQo4mpJG",
        "outputId": "2fc00dc7-3d98-40b7-8b6c-212ad00ae2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of collaborative solution: 35.69507555697578\n",
            "MAE of collaborative solution: 25.23807597654161\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Read the collaborative utility matrix from the CSV file\n",
        "df_colla = pd.read_csv('utility_matrix_collaborative_based.csv')\n",
        "\n",
        "# Read the original utility matrix\n",
        "df_original = pd.read_csv('utility_matrix.csv')\n",
        "\n",
        "# Fill the missing values in the original utility matrix with 0\n",
        "df_original.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(df_original, df_colla, squared=False)\n",
        "print(\"RMSE of collaborative solution:\", rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(df_original, df_colla)\n",
        "print(\"MAE of collaborative solution:\", mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUE7-YdfhIe6",
        "outputId": "3a53eac4-1b72-4a29-de70-e9a4ff6a0f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken:  289.66928720474243\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "# Compute the cosine similarity between each pair of items\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "item_sim = cosine_similarity(df.T)\n",
        "\n",
        "# Use the cosine similarity to estimate missing ratings\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            item_ratings = df.iloc[i, :]\n",
        "            similar_items = item_sim[j]\n",
        "            df.iloc[i, j] = (similar_items * item_ratings).sum() / similar_items.sum()\n",
        "# Compute RMSE for both solution\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_collaborative = pd.read_csv('utility_matrix_collaborative_based.csv')\n",
        "df_content = pd.read_csv('utility_matrix_content_based.csv')\n",
        "\n",
        "# RMSE for collaborative\n",
        "rmse_collaborative = mean_squared_error(df, df_collaborative, squared=False)\n",
        "\n",
        "\n",
        "# RMSE for content\n",
        "rmse_content = mean_squared_error(df, df_content, squared=False)\n",
        "\n",
        "# RMSE total\n",
        "rmse_tot = rmse_collaborative + rmse_content\n",
        "\n",
        "# Weights\n",
        "w1 = 1 - rmse_content / rmse_tot\n",
        "w2 = 1 - rmse_collaborative / rmse_tot\n",
        "\n",
        "# Hybrid utility matrix\n",
        "Ut_hybrid = w1 * df_content + w2 * df_collaborative\n",
        "\n",
        "# Convert the matrix to a DataFrame\n",
        "df_hybrid = pd.DataFrame(Ut_hybrid)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_hybrid.to_csv('utility_matrix_hybrid.csv', index=False)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Read the hybrid utility matrix from the CSV file\n",
        "df_hybrid = pd.read_csv('utility_matrix_hybrid.csv')\n",
        "\n",
        "# Read the original utility matrix\n",
        "df_original = pd.read_csv('utility_matrix.csv')\n",
        "\n",
        "# Fill the missing values in the original utility matrix with 0\n",
        "df_original.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(df_original, df_hybrid, squared=False)\n",
        "print(\"RMSE of hybrid solution:\", rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(df_original, df_hybrid)\n",
        "print(\"MAE of hybrid solution:\", mae)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMq7U9C0hyJF",
        "outputId": "5336dc02-a51f-4e56-8ffa-44b82a735542"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of hybrid solution: 35.69507555697578\n",
            "MAE of hybrid solution: 25.23807597654161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix_hybrid.csv')\n",
        "\n",
        "# Threshold to consider a query as recommended\n",
        "threshold = 70\n",
        "\n",
        "# Create an empty dataframe to store the recommended queries\n",
        "recommendations = pd.DataFrame(columns=['user', 'query'])\n",
        "\n",
        "# Iterate over each user and find the queries with an estimated rating above the threshold\n",
        "for user in df.index:\n",
        "    recommended_queries = df.loc[user][df.loc[user] > threshold].index\n",
        "    recommendations = recommendations.append(pd.DataFrame({'user': [user]*len(recommended_queries), 'query': recommended_queries}))\n",
        "\n",
        "# Save the recommended queries to a CSV file\n",
        "recommendations.to_csv('recommended_queries.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6Tl7COpPh-oR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"utility_matrix_hybrid.csv\")\n",
        "memory_usage = df.memory_usage().sum() / 1024**2\n",
        "\n",
        "print(\"Memory usage of the data: {:.2f} MB\".format(memory_usage))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Xgp51Sl-k8",
        "outputId": "f17bc8e6-201d-428d-f23d-2ffa9b19a7da"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage of the data: 76.29 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the recommended queries\n",
        "recommendations = pd.read_csv('recommended_queries.csv')\n",
        "\n",
        "# User ID for which to print recommendations\n",
        "user_id = '50248bff-1475-4bca-81c4-f40eb17435f7'\n",
        "\n",
        "# Filter the recommendations for the specified user ID\n",
        "user_recommendations = recommendations[recommendations['user'] == user_id]\n",
        "\n",
        "# Print the recommended queries for the user\n",
        "print(user_recommendations['query'].values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eME5e6mCP5kv",
        "outputId": "f4d1166c-3a38-47bd-ed24-395ff49a36b5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKqXDvvAOyKt"
      },
      "execution_count": 31,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpEAydSSxLfZz/ljiZJR6U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}