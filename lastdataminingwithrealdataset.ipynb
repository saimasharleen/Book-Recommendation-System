{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saimasharleen/Book-Recommendation-System/blob/main/lastdataminingwithrealdataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWIdTwJ7Nwme",
        "outputId": "c5264630-b927-4914-b253-6e2b65a8e192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of column names :  ['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13', 'language_code', '  num_pages', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher']\n"
          ]
        }
      ],
      "source": [
        "#atfirst we are using the scrap dataset\n",
        "# importing the csv library\n",
        "import csv\n",
        " \n",
        "# opening the csv file by specifying\n",
        "# the location\n",
        "# with the variable name as csv_file\n",
        "with open('book_dataset.csv') as csv_file:\n",
        " \n",
        "    # creating an object of csv reader\n",
        "    # with the delimiter as ,\n",
        "    csv_reader = csv.reader(csv_file, delimiter = ',')\n",
        " \n",
        "    # list to store the names of columns\n",
        "    list_of_column_names = []\n",
        " \n",
        "    # loop to iterate through the rows of csv\n",
        "    for row in csv_reader:\n",
        " \n",
        "        # adding the first row\n",
        "        list_of_column_names.append(row)\n",
        " \n",
        "        # breaking the loop after the\n",
        "        # first iteration itself\n",
        "        break\n",
        " \n",
        "# printing the result\n",
        "print(\"List of column names : \",\n",
        "      list_of_column_names[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsVVYBRrOFmN"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "import pandas as pd\n",
        "\n",
        "user_ids = set()\n",
        "while len(user_ids) < 20000:\n",
        "    user_ids.add(str(uuid.uuid4()))\n",
        "\n",
        "# convert the set to dataframe\n",
        "user_ids_df = pd.DataFrame(list(user_ids), columns=[\"user_id\"])\n",
        "\n",
        "# write the dataframe to a csv file\n",
        "user_ids_df.to_csv(\"user_ids.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsOt6pkHONln"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import uuid\n",
        "\n",
        "# Column names of the book dataset\n",
        "column_names = ['bookID', 'title', 'authors', 'average_rating', 'isbn', 'isbn13', 'language_code', 'num_pages', 'ratings_count', 'text_reviews_count', 'publication_date', 'publisher']\n",
        "\n",
        "# Number of queries to be generated\n",
        "num_queries = 1000\n",
        "\n",
        "# List to store queries\n",
        "queries = []\n",
        "\n",
        "# Open the book dataset CSV file\n",
        "with open('book_dataset.csv', 'r') as book_file:\n",
        "    # Create a CSV reader\n",
        "    reader = csv.DictReader(book_file, fieldnames=column_names)\n",
        "    # Iterate through rows of the book dataset\n",
        "    for i, row in enumerate(reader):\n",
        "        # Generate a query ID\n",
        "        query_id = 'Q' + str(i)\n",
        "        # Create a list to store the conditions\n",
        "        conditions = []\n",
        "        # Iterate through the columns of the book dataset\n",
        "        for column_name in column_names:\n",
        "            # Skip the first column (bookID)\n",
        "            if column_name == 'bookID':\n",
        "                continue\n",
        "            # Add the condition to the list\n",
        "            conditions.append(column_name + '=' + row[column_name])\n",
        "        # Join the conditions with comma separator\n",
        "        conditions_str = ','.join(conditions)\n",
        "        # Append the query to the list\n",
        "        queries.append([query_id, conditions_str])\n",
        "        # Break the loop after generating the specified number of queries\n",
        "        if i == num_queries:\n",
        "            break\n",
        "\n",
        "# Write the queries to a CSV file\n",
        "with open('queries.csv', 'w', newline='') as query_file:\n",
        "    writer = csv.writer(query_file)\n",
        "    writer.writerows(queries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpX5_m1lORyX"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import csv\n",
        "import uuid\n",
        "\n",
        "# create the set of user IDs\n",
        "user_ids = set()\n",
        "while len(user_ids) < 10000:\n",
        "    user_ids.add(str(uuid.uuid4()))\n",
        "    \n",
        "# create the set of query IDs\n",
        "query_ids = set()\n",
        "while len(query_ids) < 5000:\n",
        "    query_ids.add(\"Q\"+str(uuid.uuid4()))\n",
        "\n",
        "# create the utility matrix\n",
        "utility_matrix = []\n",
        "\n",
        "# add the first row (query IDs)\n",
        "utility_matrix.append(list(query_ids))\n",
        "\n",
        "# add the rest of the rows (user IDs and random values)\n",
        "for user_id in user_ids:\n",
        "    row = [user_id]\n",
        "    for query_id in query_ids:\n",
        "        if random.random() < 0.8:\n",
        "            row.append(random.randint(1,100))\n",
        "        else:\n",
        "            row.append('')\n",
        "    utility_matrix.append(row)\n",
        "\n",
        "# write the utility matrix to a CSV file\n",
        "with open('utility_matrix.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerows(utility_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHG4mf9JOly1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix.csv')\n",
        "df.fillna(0, inplace=True)  # Replace NaN values with 0\n",
        "\n",
        "cosine_sim = cosine_similarity(df, dense_output=False)\n",
        "\n",
        "# Define the number of closest neighbors to use\n",
        "k = 10\n",
        "\n",
        "# Next, fill in missing values with estimated values using the cosine similarity matrix\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            user_ratings = df.iloc[i, :]\n",
        "            item_ratings = df.iloc[:, j]\n",
        "            user_ratings = user_ratings[~pd.isna(user_ratings)]\n",
        "            item_ratings = item_ratings[~pd.isna(item_ratings)]\n",
        "            # subtract the mean rating of each user and item\n",
        "            mean_user_rating = user_ratings.mean()\n",
        "            mean_item_rating = item_ratings.mean()\n",
        "            user_ratings -= mean_user_rating\n",
        "            item_ratings -= mean_item_rating\n",
        "            similar_users = cosine_sim[i][~pd.isna(df.iloc[:, j])]\n",
        "            similar_items = cosine_sim[j][~pd.isna(df.iloc[i, :])]\n",
        "            # get the k closest neighbors\n",
        "            top_k_users = similar_users.argsort()[-k:]\n",
        "            top_k_items = similar_items.argsort()[-k:]\n",
        "            similar_users = similar_users[top_k_users]\n",
        "            similar_items = similar_items[top_k_items]\n",
        "            item_ratings = item_ratings[top_k_items]\n",
        "            df.iloc[i, j] = mean_item_rating + mean_user_rating + (sum(similar_users * similar_items * item_ratings) / sum(similar_users * similar_items))\n",
        "\n",
        "# save the filled matrix to csv\n",
        "df.to_csv('utility_matrix_content_based.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WQVzmu7O66o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f01ef46e-c555-4ab9-afab-02602e5bed4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of content solution: 22.58778626075449\n",
            "MAE of content solution: 10.103592716675797\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "\n",
        "# Create an imputer with strategy 'mean'\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "  \n",
        "# Fit the imputer on the original data frame\n",
        "df_original = pd.read_csv('utility_matrix.csv')\n",
        "df_original = mean_imputer.fit_transform(df_original)\n",
        "\n",
        "# Read the content utility matrix from the CSV file\n",
        "df_con = pd.read_csv('utility_matrix_content_based.csv')\n",
        "df_con = mean_imputer.transform(df_con)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(df_original, df_con, squared=False)\n",
        "print(\"RMSE of content solution:\", rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(df_original, df_con)\n",
        "print(\"MAE of content solution:\", mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3sF0GDGWXsR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix.csv')\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# Compute the Pearson correlation coefficient between each pair of users\n",
        "user_corr = df.corr(method='pearson')\n",
        "\n",
        "# Define the number of closest neighbors to use for filling in missing values\n",
        "k = 5  # for example\n",
        "\n",
        "# Fill in missing values with estimated values using collaborative filtering with Pearson correlation\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            user_ratings = df.iloc[i, :]\n",
        "            item_ratings = df.iloc[:, j]\n",
        "            user_ratings = user_ratings[~pd.isna(user_ratings)]\n",
        "            item_ratings = item_ratings[~pd.isna(item_ratings)]\n",
        "            similar_users = user_corr[i][~pd.isna(df.iloc[:, j])]\n",
        "            # Sort the similar users by Pearson correlation coefficient\n",
        "            similar_users = similar_users.sort_values(ascending=False)\n",
        "            # Use only the top k similar users\n",
        "            similar_users = similar_users[:k]\n",
        "            df.iloc[i, j] = (similar_users * item_ratings).sum() / similar_users.abs().sum()\n",
        "\n",
        "# save the filled matrix to csv\n",
        "df.to_csv('utility_matrix_collaborative_based.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99ZhBu77WmQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc56846-6da0-4013-d710-f31a8771fdea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE of collaborative solution: 22.58778626075449\n",
            "MAE of collaborative solution: 10.103592716675797\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Read the collaborative utility matrix from the CSV file\n",
        "df_colla = pd.read_csv('utility_matrix_collaborative_based.csv')\n",
        "\n",
        "# Read the original utility matrix\n",
        "df_original = pd.read_csv('utility_matrix.csv')\n",
        "\n",
        "# Fill the missing values in the original utility matrix with 0\n",
        "df_original.fillna(0, inplace=True)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = mean_squared_error(df_original, df_colla, squared=False)\n",
        "print(\"RMSE of collaborative solution:\", rmse)\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(df_original, df_colla)\n",
        "print(\"MAE of collaborative solution:\", mae)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygQSoHRHfYHB",
        "outputId": "89073724-8742-4289-c1dc-ffa518da051b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken:  1409.2307574748993\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "# Compute the cosine similarity between each pair of items\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "item_sim = cosine_similarity(df.T)\n",
        "\n",
        "# Use the cosine similarity to estimate missing ratings\n",
        "for i in range(df.shape[0]):\n",
        "    for j in range(df.shape[1]):\n",
        "        if pd.isna(df.iloc[i, j]):\n",
        "            item_ratings = df.iloc[i, :]\n",
        "            similar_items = item_sim[j]\n",
        "            df.iloc[i, j] = (similar_items * item_ratings).sum() / similar_items.sum()\n",
        "# Compute RMSE for both solution\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "df_collaborative = pd.read_csv('utility_matrix_collaborative_based.csv')\n",
        "df_content = pd.read_csv('utility_matrix_content_based.csv')\n",
        "\n",
        "# RMSE for collaborative\n",
        "rmse_collaborative = mean_squared_error(df, df_collaborative, squared=False)\n",
        "\n",
        "\n",
        "# RMSE for content\n",
        "rmse_content = mean_squared_error(df, df_content, squared=False)\n",
        "\n",
        "# RMSE total\n",
        "rmse_tot = rmse_collaborative + rmse_content\n",
        "\n",
        "# Weights\n",
        "w1 = 1 - rmse_content / rmse_tot\n",
        "w2 = 1 - rmse_collaborative / rmse_tot\n",
        "\n",
        "# Hybrid utility matrix\n",
        "Ut_hybrid = w1 * df_content + w2 * df_collaborative\n",
        "\n",
        "# Convert the matrix to a DataFrame\n",
        "df_hybrid = pd.DataFrame(Ut_hybrid)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_hybrid.to_csv('utility_matrix_hybrid.csv', index=False)\n",
        "end = time.time()\n",
        "\n",
        "print(\"Time taken: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOTGLBnSlSic"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read in the utility matrix\n",
        "df = pd.read_csv('utility_matrix_hybrid.csv')\n",
        "\n",
        "# Threshold to consider a query as recommended\n",
        "threshold = 70\n",
        "\n",
        "# Create an empty dataframe to store the recommended queries\n",
        "recommendations = pd.DataFrame(columns=['user', 'query'])\n",
        "\n",
        "# Iterate over each user and find the queries with an estimated rating above the threshold\n",
        "for user in df.index:\n",
        "    recommended_queries = df.loc[user][df.loc[user] > threshold].index\n",
        "    recommendations = recommendations.append(pd.DataFrame({'user': [user]*len(recommended_queries), 'query': recommended_queries}))\n",
        "\n",
        "# Save the recommended queries to a CSV file\n",
        "recommendations.to_csv('recommended_queries.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIT-rGEMHPLe",
        "outputId": "2916e2ed-12ae-4484-8346-87ae86e81590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Q58a9d11e-54b5-4be7-be7e-4160327f052f',\n",
            "       'Q098e1015-d0a4-4484-806e-600a0839ff4e',\n",
            "       'Qfd144bd7-b961-4b8f-99df-8d889f191e30',\n",
            "       'Q8bba3f87-da82-48f1-9839-ee5364e75e54',\n",
            "       'Q2c9394e2-5e38-456e-9376-6a3fa4d8548e',\n",
            "       'Q528c086a-719a-4464-8435-9a213c1a6a62',\n",
            "       'Qd38b9290-9ce3-476d-a41b-dc6c5444d5ad',\n",
            "       'Q27b098a3-b6f7-478c-8466-e0bffc858bfb',\n",
            "       'Q5eb1fa43-37ea-46ca-b928-176744b5d75e',\n",
            "       'Q891a939c-e866-4a43-9e5a-804b8ff8e9f3',\n",
            "       ...\n",
            "       'Q49a67109-ebc4-4311-bc99-c5bf7ec23198',\n",
            "       'Q38dd6f98-b522-4e4f-baec-1a1c34831aa9',\n",
            "       'Q86dccc12-9fa5-456c-90b5-d92bd4c66ece',\n",
            "       'Q69b6ce85-f5c5-41f2-b2f0-01105e8121d1',\n",
            "       'Qcedc2d8e-637b-4275-81e4-37e6c88db055',\n",
            "       'Q60cd6e5d-f17a-4c10-9f36-fa4cc8239144',\n",
            "       'Q2619eb65-4bd5-4975-86e1-bfe49f61994a',\n",
            "       'Q5963729d-fa24-496d-bc3f-236706313deb',\n",
            "       'Q72931202-8340-4d7a-903a-c432a2e718da',\n",
            "       'Q4a63579d-1a8b-444d-866f-325049aceff1'],\n",
            "      dtype='object', length=5000)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('utility_matrix_hybrid.csv')\n",
        "print(df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqj1f2oeJboN",
        "outputId": "dd746ad4-b6a1-4516-9833-8d8a116515cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage of the utility of a query: 0.37 MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"utility_matrix_hybrid.csv\")\n",
        "memory_usage = df.memory_usage().sum() / 1024**3\n",
        "\n",
        "print(\"Memory usage of the utility of a query: {:.2f} MB\".format(memory_usage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8N8BKaPYGbM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTyf/FRBRHZC4j9BNlv2hw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}